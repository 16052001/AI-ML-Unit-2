---
layout: full
---

## Syllabus<a name="syllabus"></a>

Program Outline *(Tentative)*: 

**Intro:**
 - What is Machine Learning (ML)?
 - From rule-based systems to learning taking decisions
 - Type of problems we can solve with ML
 - Basic Statistics, Recap of Linear Algebra and Probability Theory
 - Multivariate Gaussian distribution, Mahalanobis distance, L_p norm
 - Correlation vs Causality
 - The “curse” of dimensionality and the manifold assumption. Unintuitive properties of high-dimensional geometry.
 
**Unsupervised Learning:**
- Dimensionality Reduction: Principal Components Analysis (PCA), t-SNE
- Clustering, Kmeans, Expectation-Maximization (EM)
- Gaussian Mixture Model (GMM)

**Supervised Learning:**
 - Regression vs Classification
 - Non-parametric models: The Nearest Neighbour (NN) Classifier, Decision Trees/Random Forest
 - Polynomial Curve Fitting
 - Parametric, Linear models: Linear regression, Least-Squares, Logistic regression, Perceptron, 
 - Gradient Descent
 - Model complexity and Bias-Variance Tradeoff; Overfitting and underfitting problems; Empirical - Risk minimization, learning theory, regularization;
 - Support Vector Machines: Optimal hyperplane, margin, kernels
 -  **“Deep Learning”**: Overparametrized, non-linear models and differential programming
	-  Multilayer Perceptron
	- The backpropagation algorithm
	- Convolution over regular grids, pooling, activation functions
	- Analytical gradient and numerical with finite differences
	- Computational Graph and Automatic Differentiation
	- Stochastic Gradient Descent (SGD) over mini-batches
	- DNN parameters estimation for classification as MLE (maximum likelihood estimation)
	- Loss function for classification: softmax+cross-entropy loss, information-theory interpretation.

**Toolsets**: Python, NumPy (matrix manipulation and linear algebra), scikit learn (basic ML), matplotlib (visualization), PyTorch (automatic differentiation and neural nets).
